{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wander03/CSC-369---Lab-4/blob/main/CSC_369_PySpark_Introduction_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQ9QCIhsShu"
      },
      "source": [
        "# Introduction to Apache Spark / PySpark\n",
        "\n",
        "These examples are based on [Spark 3.3.0](https://spark.apache.org/docs/3.3.0/)  \n",
        "\n",
        "Reference/API Links\n",
        "\n",
        "\n",
        "*   [Apache Spark Quick Start](https://spark.apache.org/docs/3.3.0/quick-start.html)\n",
        "*   [PySpark v3.3.0 API](https://spark.apache.org/docs/3.3.0/api/python/reference/index.html)\n",
        "*    [RDD Programming Guide](https://spark.apache.org/docs/3.3.0/rdd-programming-guide.html)\n",
        "*    [Spark SQL Programming Guide](https://spark.apache.org/docs/3.3.0/sql-programming-guide.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoolY63aauhH"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad7fmwTbMwW"
      },
      "source": [
        "\n",
        "# Imports / Starter Example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKUgYFX9olK5"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql import types as sparktypes\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "sc = SparkContext() \n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z53FFQEDayTj"
      },
      "source": [
        "# create a Resilient Distributed Dataset (RDD) from a sequence of integers perform filter() and reduce() operations\n",
        "\n",
        "# Function to be used in the filter() transformation\n",
        "def filterSmall(x):    \n",
        "   if x < 20:\n",
        "      return False\n",
        "   else:\n",
        "      return True\n",
        "\n",
        "# Function to be used in the map() transformation\n",
        "def mapSquare(x):\n",
        "    return x*x\n",
        "\n",
        "# Function to be used in the reduce() action\n",
        "def reduceSum(x,y):\n",
        "    return x+y\n",
        "  \n",
        "rdd = sc.parallelize(range(100))         ## create an RDD of 100 numbers from 0 to 99\n",
        "\n",
        "#print(rdd.filter(filterSmall).collect())\n",
        "\n",
        "out1 = rdd.filter(filterSmall).map(mapSquare)  ## perform filter and map transformations\n",
        "out2 = out1.reduce(reduceSum)                  ## perform reduce operation\n",
        "\n",
        "print(out1.collect())           ## print first output (all numbers less than 20 squared)\n",
        "print(out2)                 ## print second output (sum of all numbers from first output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slRw9-jLsHOd"
      },
      "source": [
        "# download a sample access log for use in demos below\n",
        "!rm -f apache.access.log\n",
        "!wget -q https://raw.githubusercontent.com/databricks/reference-apps/master/logs_analyzer/data/apache.access.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmAntGt8iNli"
      },
      "source": [
        "# Apache HTTP Log Example - Resilient Distributed Dataset (RDD)\n",
        "\n",
        "A SparkContext instance can be used to create RDDs from various data/files/resources (text files, CSV, Hadoop data files, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUbUL01ff2j5"
      },
      "source": [
        "# find the top 10 clients, map/reduce style using RDD transformations\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[0], 1 ))  # field 0 = client address\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "                  .sortBy(lambda t: -t[1])) \n",
        "\n",
        "print (\"Total count of client hostnames:\")\n",
        "print(access_log_rdd.count())\n",
        "\n",
        "print (\"Top 10 client hostnames:\")\n",
        "print(access_log_rdd.take(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ca1vYvspy6_"
      },
      "source": [
        "# Apache HTTP Log Example - DataFrame\n",
        "\n",
        "A DataFrame is equivalent to a relational table in Spark SQL, and can be created from on a variety of input formats (CSV, JSON, relational database, etc.) using the SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wccwGjPmBBL"
      },
      "source": [
        "access_log_df = spark.read.text(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqc5g2nNqq3e"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuD2cLn6p_Ob"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "\n",
        "\n",
        "\n",
        "named_df.show(truncate=False)\n",
        "named_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdpYRf3z9y_h"
      },
      "source": [
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"SELECT * FROM log WHERE status = 404\")\n",
        "\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBrUNHSBtgv"
      },
      "source": [
        "## What About Datasets?\n",
        "\n",
        "Added in Spark 1.6, a **Dataset** is a distributed collection of data that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (`map`, `flatMap`, `filter`, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
        "\n",
        "A **DataFrame** is a Dataset organized into named columns. ([source](https://spark.apache.org/docs/3.1.1/sql-programming-guide.html)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ2zn6ezgTmF"
      },
      "source": [
        "# Reporting Tasks (from Lab 2)\n",
        "\n",
        "\n",
        "1. Most popular URL paths (top 15)\n",
        "2. Request count for each HTTP response code, sorted by response code\n",
        "3. Request count for each calendar month and year, sorted chronologically\n",
        "4. Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "5. Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPvNuJGgkQl"
      },
      "source": [
        "# (A) RDD Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using RDD transformations\n",
        "\n",
        "[RDD APIs PySpark v3.3.0](https://spark.apache.org/docs/3.3.0/api/python/reference/pyspark.html#rdd-apis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNTpq69g1pU"
      },
      "source": [
        "# RDD implementation\n",
        "# (1) Most popular URL paths (top 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az250Dgghvd5"
      },
      "source": [
        "# RDD implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa17VZSyhv4X"
      },
      "source": [
        "# RDD implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSUdQb4Mhv-W"
      },
      "source": [
        "# RDD implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi89G8EQhwEO"
      },
      "source": [
        "# RDD implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxkjTvxIhBO-"
      },
      "source": [
        "# (B) DataFrame Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark's DataFrame API\n",
        "\n",
        "[DataFrame API PySpark v.3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
        "\n",
        "Please Note: Spark SQL **is not** permitted for these exercises. You must use the Spark DataFrame API. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGYTdWLhJCH"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (1) Most popular URL paths (top 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apq4hUSVh94J"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSvjvCNth9-S"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjkoHWOxh-Df"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwpDYxTh-Iv"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}